---
title: "Mini-Project 2: Adventure 2"
author: Jack Tan, Debbie Sun, Alex Denzler, Phuong Nguyen
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

## Part 1: Background
### What is an SVM?
A Support Vector Machine is a supervised machine learning algorithm that is suitable for both classification and regression. For a given set of data that is marked as one category or the other of a total of two categories, an SVM algorithm builds a model that assigns new examples to either of the two categories, making it a binary linear classifier. In an SVM model, cases of the dataset are points in a p-dimensional space, mapped so that they are separated by a p-1 dimension hyperplane with maximized gap. New cases are predicted by assigning each new case to either side of the hyperplane.

### What is a hyperplane?

The hyperplane is a flat affine subspace of demension p-1 in a p dimension space. The goal of the hyperplane is to classify the data points into different categories correctly. The natural choice of hyperplane is maximal margin hyperplane. We calculate the distance of each observation to the hyperplane and define the minimum of the distance as the margin of the hyperplane. There are many hyperplanes but we want the one maximizes the margin. The intuition behind this is that the maximal margin hyperplane can give us a better split because it makes each group/category more distinct from each other by using the maximum of the margin, and our classification are more likely to have a high accuracy. 

### Details of hyperplane

In reality, we probably can't make a perfect split because of the non-separable cases when observations from different categories mix with each other. There are errors when we are making classfiction for each observation. The error term is defined in 3 ways. If the observation is on the wrong side of the margin but the right side of the hyperplane, $\varepsilon_i > 0$, if the observation is the on the wrong side of the hyperplane, then $\varepsilon_i > 1$; if the observation is perfectly classfied, then $\varepsilon_i = 0$. Therefore, we also need to define a term of tolerance for the errors, and we define C as the sum of all errors. It determines the severity and number of violations that we will tolerate. Since we choose the level of tolerance, C is our tuning parameter in the SVM algorithm. 






### Pros and cons of SVM.
Pros:
- The solutions given by SVM are guaranteed to be global minimum instead of locally minimum, due to the nature of the optimization of the method.
- SVM is a spectacular method that is suitable for both linearly and non-linearly separable data sets by using kernel tricks. The only thing to do is to tune the tuning parameter C.
- SVM works well with data sets that have low dimentionality as well as data sets that have high dimensionality. The algorithm works well with high-dimension because the complexity of training data set in SVM is generally characterized by the support vectors rather than the dimension of the data set.
- SVM can work effectively on smaller training data set because it does not rely on the entire data.

Cons:
- SVM is really computationally expensive and thus can not work with large data set.
- SVM is not so good at dealing with data sets that have multiple overlapping classes.

\
\
\
\
\
\



## Part 2: 2 predictor analysis
```{r, message = FALSE}
library(caret)
library(e1071)
library(dplyr)
library(kernlab)
library(car)
library(openxlsx)
library(VIM)
```

```{r}
kangaroo <- read.csv("https://www.macalester.edu/~ajohns24/data/kangaroo.csv")
set.seed(253)
kangaroo_selected <- kangaroo %>% 
  filter(species != c("melanops")) %>%
  mutate_if(is.factor, ~droplevels(.))
levels(kangaroo_selected$species)
```

```{r}
#set the seed
set.seed(253)

#Tuning parameter c
c <- seq(0.01, 3, length = 200)
grid <- expand.grid(sigma = c(.01, .015, 0.2), C = c(0.75, 0.9, 1, 1.1, 1.25))

#svm model
svm_model <- train(
  species ~ zygomatic.width + nasal.width,
  data = kangaroo_selected,
  method = "svmLinear",
  trControl=trainControl(method = "cv", number = 10, selectionFunction = "best"),
  preProcess = c("center", "scale"),
  metric = "Accuracy",
  tuneGrid = data.frame(C = c),
  na.action = na.omit
  )

#svm polynomial model
svm_poly <- train(
  species ~ zygomatic.width + nasal.width,
  data = kangaroo_selected,
  method = "svmPoly",
  trControl=trainControl(method = "cv", number = 10, selectionFunction = "best"),
  preProcess = c("center", "scale"),
  metric = "Accuracy",
  tuneGrid = data.frame(C = c, scale = TRUE, degree = seq(1, 5, length = 5)),
  na.action = na.omit
  )

```


```{r}
plot(svm_model)
plot(svm_poly)
```

```{r}
svm_model$results %>% 
  filter(C == svm_model$bestTune$C)
svm_poly$results %>%
  filter(C == svm_poly$bestTune$C)
```


\
\
\
\
\
\



## Part 3: Full analysis




\
\
\
\
\
\



## Part 4: Summarize






\
\
\
\
\
\



## Part 5: Contributions

