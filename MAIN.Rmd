---
title: "Mini-Project 2: Adventure 2"
author: Jack Tan, Debbie Sun, Alex Denzler, Phuong Nguyen
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---
```{r, message = FALSE}
library(caret)
library(e1071)
library(dplyr)
library(kernlab)
library(car)
library(openxlsx)
library(ggplot2)
library(png)
```

## Part 1: Background

### What is an SVM?

A Support Vector Machine is a supervised machine learning algorithm that is suitable for both classification and regression. For a given set of data that is marked as one category or the other of a total of two categories, an SVM algorithm builds a model that assigns new examples to either of the two categories, making it a binary linear classifier. In an SVM model, cases of the dataset are points in a p-dimensional space, mapped so that they are separated by a p-1 dimension hyperplane with maximized gap. New cases are predicted by assigning each new case to either side of the hyperplane.

### What is a hyperplane?

The hyperplane is a flat affine subspace of demension p-1 in a p dimension space. The goal of the hyperplane is to classify the data points into different categories correctly. The natural choice of hyperplane is maximal margin hyperplane. We calculate the distance of each observation to the hyperplane and define the minimum of the distance as the margin of the hyperplane. There are many hyperplanes but we want the one maximizes the margin. The intuition behind this is that the maximal margin hyperplane can give us a better split because it makes each group/category more distinct from each other by using the maximum of the margin, and our classification are more likely to have a high accuracy. 

### Details of hyperplane

In reality, we usually can't make a perfect split because datasets might just not be linearly separable when observations from different categories mix with each other. Here we introduce the term slack variable $\zeta_i$ which allow some observations to fall off the margin but it penalizes them. Each slack variable is determined by the distance between each misclassified observation and the correct margin. In this scenario, our algorithm tries to keep $\zeta_i$ to zero while maximizing the margin. The tolerance term is defined as $ùê∂\Sigma\zeta_i$, where $\Sigma\zeta_i$ is the sum of the distance from each misclassified observation to the correct margin and $ùê∂$ is the regularization parameter that controls the trade-off between the slack variable penalty (misclassifications) and width of the margin(just like LASSO!).

### Tuning the regularization parameter

What does the $ùê∂$ parameter do in SVM classification? It tells the algorithm how much you care about misclassified points. SVMs, in general, seek to find the maximum-margin hyperplane. That is, the line that has as much room on both sides as possible. A high value for $ùê∂$ means that we want the misclassification to be punished hard, which tells the algorithm that we care more about classifying all of the training points correctly than leaving wiggle room for future data, thus the algorithm will produce small margin with low tolerance(low variance, high bias). A low value for $ùê∂$ means that we want the misclassification to be hardly punished, which tells the algorithm that we care more about leaving room for future data than classify all of the training points correctly, thus the algorithm will generate a larger margin with higher tolerance(high variance, low bias). To be more precise, when we try to increase $ùê∂$, we are betting that our training dataset contains the most extreme cases and future data will be further from the boundary than the cases our training dataset contains. Vice versa. The best $ùê∂$ is usually determined using CV.

### What is a kernel?

### Tuning parameter of a kernel function, $\gamma$

### Pros and cons of SVM.

Pros:
- The solutions given by SVM are guaranteed to be global minimum instead of locally minimum, due to the nature of the optimization of the method.
- SVM is a spectacular method that is suitable for both linearly and non-linearly separable data sets by using kernel tricks. The only thing to do is to tune the tuning parameter C.
- SVM works well with data sets that have low dimentionality as well as data sets that have high dimensionality. The algorithm works well with high-dimension because the complexity of training data set in SVM is generally characterized by the support vectors rather than the dimension of the data set.
- SVM can work effectively on smaller training data set because it does not rely on the entire data.

Cons:
- SVM is really computationally expensive and thus can not work with large data set.
- SVM is not so good at dealing with data sets that have multiple overlapping classes.

### Comparison among SVM and other algorithm


\
\
\
\
\
\

## Part 2: 2 predictor analysis
**Ready the data set**
```{r}
kangaroo <- read.csv("https://www.macalester.edu/~ajohns24/data/kangaroo.csv")
set.seed(253)
kangaroo_selected <- kangaroo %>% 
  filter(species != c("melanops")) %>%
  mutate_if(is.factor, ~droplevels(.))
levels(kangaroo_selected$species)
```

**visualization of data**
```{r warning  = FALSE}
ggplot(data = kangaroo_selected, aes(x = zygomatic.width, y = nasal.width, color = species)) + 
  geom_point()
```


**Two-model build up**
```{r}
#set the seed
set.seed(253)

#Tuning parameter c
c <- seq(0.01, 3, length = 200)
grid <- expand.grid(sigma = c(.01, .015, 0.2), C = c(0.75, 0.9, 1, 1.1, 1.25))

#svm model
svm_model <- train(
  species ~ zygomatic.width + nasal.width,
  data = kangaroo_selected,
  method = "svmLinear",
  trControl=trainControl(method = "cv", number = 10, selectionFunction = "best"),
  preProcess = c("center", "scale"),
  metric = "Accuracy",
  tuneGrid = data.frame(C = c),
  na.action = na.omit
  )

#svm polynomial model
svm_poly <- train(
  species ~ zygomatic.width + nasal.width,
  data = kangaroo_selected,
  method = "svmPoly",
  trControl=trainControl(method = "cv", number = 10, selectionFunction = "best"),
  preProcess = c("center", "scale"),
  metric = "Accuracy",
  tuneGrid = data.frame(C = c, scale = TRUE, degree = seq(1, 5, length = 5)),
  na.action = na.omit
  )

```

**Accuracy plots for the two models**
```{r}
plot(svm_model)
plot(svm_poly)
```

**The best tuning parameter**
```{r}
svm_model$results %>% 
  filter(C == svm_model$bestTune$C)
svm_poly$results %>%
  filter(C == svm_poly$bestTune$C)
```


\
\
\
\
\
\



## Part 3: Full analysis




\
\
\
\
\
\



## Part 4: Summarize






\
\
\
\
\
\



## Part 5: Contributions

