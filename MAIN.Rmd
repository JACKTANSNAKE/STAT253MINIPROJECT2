---
title: "Mini-Project 2: Adventure 2"
author: Jack Tan, Debbie Sun, Alex Denzler, Phuong Nguyen
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

## Part 1: Background
### What is an SVM?
A Support Vector Machine is a supervised machine learning algorithm that is suitable for both classification and regression. For a given set of data that is marked as one category or the other of a total of two categories, an SVM algorithm builds a model that assigns new examples to either of the two categories, making it a binary linear classifier. In an SVM model, cases of the dataset are points in a p-dimensional space, mapped so that they are separated by a p-1 dimension hyperplane with maximized gap. New cases are predicted by assigning each new case to either side of the hyperplane.

### What is a hyperplane?










### Pros and cons of SVM.
Pros:
- The solutions given by SVM are guaranteed to be global minimum instead of locally minimum, due to the nature of the optimization of the method.
- SVM is a spectacular method that is suitable for both linearly and non-linearly separable data sets by using kernel tricks. The only thing to do is to tune the tuning parameter C.
- SVM works well with data sets that have low dimentionality as well as data sets that have high dimensionality. The algorithm works well with high-dimension because the complexity of training data set in SVM is generally characterized by the support vectors rather than the dimension of the data set.
- SVM can work effectively on smaller training data set because it does not rely on the entire data.

Cons:
- SVM is really computationally expensive and thus can not work with large data set.
- SVM is not so good at dealing with data sets that have multiple overlapping classes.

\
\
\
\
\
\



## Part 2: 2 predictor analysis
```{r, message = FALSE}
library(caret)
library(e1071)
library(dplyr)
library(kernlab)
library(car)
library(openxlsx)
library(VIM)
```

```{r}
kangaroo <- read.csv("https://www.macalester.edu/~ajohns24/data/kangaroo.csv")
set.seed(253)
kangaroo_selected <- kangaroo %>% 
  filter(species != c("melanops")) %>%
  mutate_if(is.factor, ~droplevels(.))
levels(kangaroo_selected$species)
```

```{r}
#set the seed
set.seed(253)

#Tuning parameter c
c <- seq(0.01, 3, length = 200)
grid <- expand.grid(sigma = c(.01, .015, 0.2), C = c(0.75, 0.9, 1, 1.1, 1.25))

#svm model
svm_model <- train(
  species ~ zygomatic.width + nasal.width,
  data = kangaroo_selected,
  method = "svmLinear",
  trControl=trainControl(method = "cv", number = 10, selectionFunction = "best"),
  preProcess = c("center", "scale"),
  metric = "Accuracy",
  tuneGrid = data.frame(C = c),
  na.action = na.omit
  )

#svm polynomial model
svm_poly <- train(
  species ~ zygomatic.width + nasal.width,
  data = kangaroo_selected,
  method = "svmPoly",
  trControl=trainControl(method = "cv", number = 10, selectionFunction = "best"),
  preProcess = c("center", "scale"),
  metric = "Accuracy",
  tuneGrid = data.frame(C = c, scale = TRUE, degree = seq(1, 5, length = 5)),
  na.action = na.omit
  )

```


```{r}
plot(svm_model)
plot(svm_poly)
```

```{r}
svm_model$results %>% 
  filter(C == svm_model$bestTune$C)
svm_poly$results %>%
  filter(C == svm_poly$bestTune$C)
```


\
\
\
\
\
\



## Part 3: Full analysis




\
\
\
\
\
\



## Part 4: Summarize






\
\
\
\
\
\



## Part 5: Contributions

